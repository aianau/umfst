Analiza unui set mare de date (la alegere) folosind tehnicile prezentate la curs.
 - Descrieți pe scurt datele analizate.
 - Descrieți ipotezele studiate.
 - Descrieți metodele de analiză folosite.
 - Comparați eficiența diferitelor implementări din punct de vedere al timpului de rulare, respectiv ,memoria utilizată..
 - Reprezentați grafic rezultatele.
 - Interpretați rezultatele obținute.
 
Pentru partea a doua a proiectului se va folosi o baza de date NOSQL (de exemplu MongoDB) pentru stocarea datelor și procesarea lor server side, și se vor adăga elemente de procesare paralelă. La fel accentul se pune pe compararea mai multor metode și evaluarea performanțelor.


Proiectele la Ştiința Datelor folosind Python:

Proiectul 1 - deadline 7.03.2023

Realizați un proiect care compară mai multe elemente ale unui tablou mare (bază de date). Baza de date să aibă aproximativ 10000 linii x 10000 coloane, iar analiza datelor să presupună o analiză statistică, realizarea de comparații, măsurarea timpului de lucru necesar şi a memoriei consumate. 

Proiectul 2 - deadline 28.03.2023

Realizați un proiect care poate fi o extindere a primului proiect sau altul complet diferit, folosind neapărat baza de date MongoDB, prin metode de analiză propuse de voi.


-- fiecare idee (trb 4-5 idei) trebuie realizate in 2 metode si comparate dpdv al eficientei de memorie+procesare si de ce merge asa


alte idei pe care le-a mai zis profa:
* poti sa iei din mai multe surse de date si sa le faci merging. 
* ca idee poti sa faci adaugarea in dataframe/Series element cu element si apoi in batch si sa compari eficienta proc+mem.
* in structurile clasice de programare, poti filtra rows pe elemente (da-mi intrarile cu cell == "albastru"). Dar aici poti cu MultiIndex.
* trb elemente de agregare a tablouri. comparare cu mai multe posibilitati.
* alte intrebare pe care incercam sa o raspundem e: 
** pt un df ft ft mare, e mai util sa cream o noua coloana in acel df sau doar sa facem un Series separat, sa ne jucam cu el si apoi il lasam sa fie sters?
fiindca memoria pastrata poate nu merita pt procesarea pe care o castigam




Pasi:
* analiza asupra datelor raw si preprocesarea
* structurarea in multiIndex si/sau organizarea pt a ne fi cat mai usor.


